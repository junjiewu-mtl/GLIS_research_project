## Junjie Wu  
## China Survey Analysis

rm(list = ls())

pacman::p_load(data.table, bit64, openxlsx, haven, dplyr, corrplot, zoo, 
               matrixStats, plotly, DAAG,PerformanceAnalytics, 
               BiocManager, ISLR, tree, rpart,rpart.plot, arsenal)

setwd('~/Desktop/GLIS Research/GLIS_research_project/data')

###################### missing value imputation #####################
# dt <- fread('cleaned_01_16_cn.csv')
# #knn
# if(exists(".Random.seed")) rm(.Random.seed)
# 
# dt_imputed <- impute.knn(as.matrix(dt[,c(2:30, 38:69, 71)]))
# dt_q30 <- dt[,c(1,31:37)]
# 
# dt_imputed <- data.table(dt_imputed$data)
# dt_imputed$V1 <- seq.int(nrow(dt_imputed))
# 
# dt_merge <- merge(dt_imputed, dt_q30, by = 'V1')
# 
# dt_merged <- dt_merge[,c(1:30,64:70,31:63)]
# 
# write.csv(dt_merged, 'knn_imputed_cn_data.csv', row.names = F)

# ####################################################################
dt <- fread('knn_imputed_cn_data.csv')[,-1]

# dt_csv_mean <- sapply(dt[,37:56], mean, na.rm=TRUE)
# 
# 
# View(sort(dt_csv_mean, decreasing = T))


# sustainable value
dt[, `:=`(sus_val = rowSums(.SD, na.rm=T)), .SDcols=c(41,49:56)]

# aesthetic values
dt[, aes_val := rowSums(.SD, na.rm=T), .SDcols=c(42:43)]

#Green Consciousness
dt[, green_con := rowSums(.SD, na.rm=T), .SDcols=c(1:5)]

#Consumer Innovativeness
dt[, con_inno := rowSums(.SD, na.rm=T), .SDcols=c(50:56, 58:62)]

################### scatter plot function: ggplotRegression ################### 
ggplotRegression <- function (fit) {
  
  require(ggplot2)
  
  ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + 
    geom_point() +
    stat_smooth(method = "lm", col = "red") +
    labs(title = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
                       "Intercept =",signif(fit$coef[[1]],5 ),
                       " Slope =",signif(fit$coef[[2]], 5),
                       " P =",signif(summary(fit)$coef[2,4], 5)))
}

#descriptive analysis

dt_mean <- sapply(dt[,38:57], mean, na.rm=TRUE)

summary(dt)
sort(dt_mean)

table_one <- tableby(~.,dt)
summary(table_one,text=TRUE)
#write.table(table_one, '../script/cn_preliminary_analysis/cn_h1.csv')

################################################################################ 


######################### Q1 ###############################

dt_individual <- dt[, c(65,41,49:56,42:43)]
dt_individual <- dt_individual[Q41.Sex < 3,]
dt_individual$Q41.Sex <- as.factor(ifelse(dt_individual$Q41.Sex<=1, "Male", "Female"))
#write.csv(dt_individual, '../script/cn_preliminary_analysis/cn_h1_ind.csv', row.names = F)

dt_aes_sus  <- dt[, c(65,70,71)]
dt_aes_sus <- dt_aes_sus[Q41.Sex < 3,]
dt_aes_sus$Q41.Sex <- as.factor(ifelse(dt_aes_sus$Q41.Sex<=1, "Male", "Female"))
#write.csv(dt_aes_sus, '../script/cn_preliminary_analysis/cn_h1_sum.csv', row.names = F)

dt_a <- dt_individual

## 75% of the sample size
smp_size <- floor(0.75 * nrow(dt_a))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(dt_a)), size = smp_size)

train <- dt_a[train_ind, ]
test <- dt_a[-train_ind, ]

# regression tree

rtree.dt_a = rpart(Q41.Sex ~ ., data=dt_a, method = 'class')
# Plot the tree using prp command defined in rpart.plot package
prp(rtree.dt_a)

t_pred = predict(rtree.dt_a,test,type="class")

confMat <- table(test$Q41.Sex,t_pred)
accuracy <- sum(diag(confMat))/sum(confMat)

mean(test$Q41.Sex == t_pred) 

summary(rtree.dt_a)

tree.dt_a = tree(Q41.Sex~., data=dt_a)

summary(tree.dt_a)

plot(tree.dt_a)
text(tree.dt_a, pretty = 0)

tree.pred = predict(tree.dt_a, dt_a[-train_ind, ], type="class")

with(dt_a[-train_ind,], table(tree.pred, Q41.Sex))

# 
# missmap(dt, main = Missing values vs observed)
# 
# #drop NA 
# dt_a <- data.table(dt_a[complete.cases(dt_a),])
# 
# dt_a <- dt_a[Q41.Sex < 3]
# dt_a[, Sex := Q41.Sex - 1]
# 
# correlations <- cor(dt_a[,1:7])
# corrplot(correlations, method=circle)
# 
# 
# ## 75% of the sample size
# smp_size <- floor(0.75 * nrow(dt_a))
# 
# ## set the seed to make your partition reproducible
# set.seed(123)
# train_ind <- sample(seq_len(nrow(dt_a)), size = smp_size)
# 
# train <- dt_a[train_ind, ]
# test <- dt_a[-train_ind, ]
# 
# # dt_a <- lapply(dt_a, as.factor)
# # dt_a[Q41.Sex] <- as.numeric(dt_a[Q41.Sex])
# 
# mylogit <- glm(Sex ~ Q31a.Fit + Q31c.Fibre.Material + Q31d.Quality.Workmanship + Q31f.Colour + Q31g.Style, data = train, family = binomial)
# 
# summary(mylogit)
# 
# fitted.results <- predict(mylogit,newdata=subset(test,select=c(2,3,4,5,6,7)),type='response')
# fitted.results <- ifelse(fitted.results > 0.5,1,0)
# 
# misClasificError <- mean(fitted.results != test$Sex)
# print(paste('Accuracy',1-misClasificError))
# 
# 
# confint(mylogit)

######################### Q2 ###############################

dt_2 <- dt
dt_2[, sus_aes := sus_val+aes_val]

dt_2 <- dt_2[,c(73,74)]

write.csv(dt_2, '../script/cn_preliminary_analysis/cn_h2.csv', row.names = F)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(dt_2))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(dt_2)), size = smp_size)

train <- dt_2[train_ind, ]
test <- dt_2[-train_ind, ]

linear_2 <- lm(con_inno ~ sus_aes, data = train)
summary(linear_2)

p_dt <- predict(linear_2, test)

actuals_preds <- data.frame(cbind(actuals=test$con_inno, predicteds=p_dt))
correlation_accuracy <- cor(actuals_preds) 
head(actuals_preds)

ggplotRegression(linear_2)



######################### Q3 ###############################

dt_3 <- dt[,c(70,72)]

write.csv(dt_3, '../script/cn_preliminary_analysis/cn_h3.csv', row.names = F)
## 75% of the sample size
smp_size <- floor(0.75 * nrow(dt_3))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(dt_3)), size = smp_size)

train <- dt_3[train_ind, ]
test <- dt_3[-train_ind, ]

liner_3 <- lm(green_con ~ sus_val, data = dt_3)
summary(liner_3)


ggplotly(ggplotRegression(liner_3))

p_dt <- predict(liner_3, test)

actuals_preds <- data.frame(cbind(actuals=test$green_con, predicteds=p_dt))
correlation_accuracy <- cor(actuals_preds) 
head(actuals_preds)

# train_eva <- train[,c(65:69)]
# 
# chart.Correlation(train_eva,
#                   method="pearson",
#                   histogram=TRUE,
#                   pch=16)


plot_ly(data = dt_3, x = ~green_con, y = ~sus_val)

